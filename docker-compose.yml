services:
  rag-backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    env_file: .env
    depends_on:
      - embed
      - ocr-module
      - diagnostics
    environment:
      APP_DATA_ROOT: ${APP_DATA_ROOT:-/app_data}
      LLM_BASE_URL: ${LLM_BASE_URL:-http://${LLM_ENDPOINT:-llm_big}:8000/v1}
      LLM_API_KEY: ${LLM_API_KEY:-}
      LLM_CONTEXT_SIZE: ${LLM_CONTEXT_SIZE:-20000}
      EMBEDDING_BASE_URL: ${EMBEDDING_BASE_URL:-}
      EMBEDDING_API_KEY: ${EMBEDDING_API_KEY:-}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL}
      DATA_DIR: ${DATA_DIR:-/app_data/docs}
      INDEX_DIR: ${INDEX_DIR:-/app_data/runtime}
      DOC_STORE_PATH: ${DOC_STORE_PATH:-/app_data/rag_meta.db}
      OCR_MODULE_URL: ${OCR_MODULE_URL:-http://ocr-module:8000}
      OCR_MODULE_TIMEOUT: ${OCR_MODULE_TIMEOUT:-180}
      OCR_PARSER_KEY: ${OCR_PARSER_KEY:-mineru}
      LLM_CONTROL_URL: ${LLM_CONTROL_URL:-http://${LLM_ENDPOINT:-llm_big}:9000/control}
      OCR_CONTROL_URL: ${OCR_CONTROL_URL:-http://ocr-module:8000/control}
      GPU_PHASE_TIMEOUT: ${GPU_PHASE_TIMEOUT:-60}
      DIAGNOSTICS_URL: ${DIAGNOSTICS_URL:-http://diagnostics:9001}
    volumes:
      - ${HOST_APP_DATA:-./app_data}:${APP_DATA_ROOT:-/app_data}
    ports:
      - "${BACKEND_PORT:-8000}:8000"

  ocr-module:
    build:
      context: ./ocr_module
      dockerfile: Dockerfile
      args:
        PREFETCH_MODELS: ${PREFETCH_MODELS:-0}
    env_file: .env
    environment:
      APP_DATA_ROOT: ${APP_DATA_ROOT:-/app_data}
      DATA_DIR: ${DATA_DIR:-/app_data/docs}
      INDEX_DIR: ${INDEX_DIR:-/app_data/runtime}
      OCR_OUTPUT_DIR: ${OCR_OUTPUT_DIR:-/app_data/runtime/ocr_outputs}
      OCR_INCOMING_DIR: ${OCR_INCOMING_DIR:-/app_data/runtime/incoming}
      OCR_WARMUP_DIR: ${OCR_WARMUP_DIR:-/app_data/runtime/warmup}
      MINERU_DEVICE_MODE: ${MINERU_DEVICE_MODE:-auto}
      MINERU_PARSE_METHOD: ${MINERU_PARSE_METHOD:-auto}
      MINERU_LANG: ${MINERU_LANG:-en}
      MINERU_TABLE_ENABLE: ${MINERU_TABLE_ENABLE:-true}
      MINERU_FORMULA_ENABLE: ${MINERU_FORMULA_ENABLE:-true}
      MINERU_WARMUP_ON_STARTUP: ${MINERU_WARMUP_ON_STARTUP:-false}
      HF_HOME: ${HF_HOME:-/app_data/runtime/caches/hf}
      TRANSFORMERS_CACHE: ${TRANSFORMERS_CACHE:-/app_data/runtime/caches/hf}
      ULTRALYTICS_HOME: ${ULTRALYTICS_HOME:-/app_data/runtime/caches/ultralytics}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${HOST_APP_DATA:-./app_data}:${APP_DATA_ROOT:-/app_data}

  llm_big:
    build:
      context: .
      dockerfile: docker/llama-cpp.Dockerfile
    image: local/llama-cpp:latest

    environment:
      LLAMA_CUBLAS: "1"
      LLM_CONTROL_PORT: 9000
      LLM_SERVER_CMD: >-
        python3 -m llama_cpp.server --model /models/llm/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --model_alias default --host 0.0.0.0 --port 8000 --chat_format chatml --n_ctx ${LLM_CONTEXT_SIZE:-20000} --n_batch ${LLM_BATCH_SIZE:-256} --n_gpu_layers -1 --flash_attn true --use_mlock true --api_key ${LLM_API_KEY:-local-llm}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./models/llm:/models/llm:ro
    ports:
      - "${LLM_HOST_PORT:-8010}:8000"

  embed:
    image: local/llama-cpp:latest
    command: >
      python3 -m llama_cpp.server
      --model /models/embed/${EMBED_MODEL_FILE:-nomic-embed-text-v1.5.Q8_0.gguf}
      --model_alias ${EMBEDDING_MODEL:-text-embedding-nomic-embed-text-v1.5}
      --host 0.0.0.0
      --port 8080
      --embedding true
      --n_ctx ${EMBED_CONTEXT_SIZE:-2048}
      --n_gpu_layers -1
      --use_mlock true
      --api_key ${EMBEDDING_API_KEY:-local-embed}
    environment:
      LLAMA_CUBLAS: "1"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ${EMBED_MODEL_DIR:-./models/embed}:/models/embed:ro
    ports:
      - "${EMBED_HOST_PORT:-8011}:8080"

  diagnostics:
    build:
      context: ./diagnostics
      dockerfile: Dockerfile
    env_file: .env
    environment:
      PORT: ${DIAGNOSTICS_PORT:-9001}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "${DIAGNOSTICS_HOST_PORT:-9001}:9001"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_LLM_CONTEXT_SIZE: ${LLM_CONTEXT_SIZE:-20000}
    depends_on:
      - rag-backend
    ports:
      - "${FRONTEND_PORT:-5173}:80"
