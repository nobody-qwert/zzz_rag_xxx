services:
  rag-backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      args:
        PREFETCH_MODELS: ${PREFETCH_MODELS:-0}
    env_file: .env
    depends_on:
      - llm-qwen
      - embed-gemma
    environment:
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      LLM_BASE_URL: ${LLM_BASE_URL:-}
      LLM_API_KEY: ${LLM_API_KEY:-}
      EMBEDDING_BASE_URL: ${EMBEDDING_BASE_URL:-}
      EMBEDDING_API_KEY: ${EMBEDDING_API_KEY:-}
      LLM_MODEL: ${LLM_MODEL}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL}
      DATA_DIR: ${DATA_DIR:-/data}
      INDEX_DIR: ${INDEX_DIR:-/indices}
      MINERU_DEVICE_MODE: ${MINERU_DEVICE_MODE:-auto}
      # Cache locations (persisted via indices volume)
      HF_HOME: /indices/hf_cache
      TRANSFORMERS_CACHE: /indices/hf_cache
      ULTRALYTICS_HOME: /indices/ultralytics
      # Parser controls
      PARSER_MODE: ${PARSER_MODE:-mineru}
      MIN_PYMUPDF_CHARS_PER_PAGE: ${MIN_PYMUPDF_CHARS_PER_PAGE:-300}
      MINERU_PARSE_METHOD: ${MINERU_PARSE_METHOD:-auto}
      MINERU_LANG: ${MINERU_LANG:-en}
      MINERU_TABLE_ENABLE: ${MINERU_TABLE_ENABLE:-true}
      MINERU_FORMULA_ENABLE: ${MINERU_FORMULA_ENABLE:-true}
      MINERU_WARMUP_ON_STARTUP: ${MINERU_WARMUP_ON_STARTUP:-false}
    # Requires NVIDIA Container Toolkit installed on host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./data:${DATA_DIR:-/data}
      - ./indices:${INDEX_DIR:-/indices}
    ports:
      - "${BACKEND_PORT:-8000}:8000"

  llm-qwen:
    build:
      context: .
      dockerfile: docker/llama-cpp.Dockerfile
    image: local/llama-cpp:latest
    command: >
      python3 -m llama_cpp.server
      --model /models/llm/${LOCAL_QWEN_MODEL_FILE:-Qwen3-4B-Instruct-2507-Q8_0.gguf}
      --model_alias ${LLM_MODEL:-qwen/qwen3-4b-2507}
      --host 0.0.0.0
      --port 8000
      --chat_format chatml
      --n_ctx ${LLM_CONTEXT_SIZE:-10000}
      --n_gpu_layers -1
      --use_mlock true
      --api_key ${LLM_API_KEY:-local-llm}
    environment:
      LLAMA_CUBLAS: "1"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ${LOCAL_QWEN_MODEL_DIR:-./models/llm}:/models/llm:ro
    ports:
      - "${LLM_HOST_PORT:-8010}:8000"

  embed-gemma:
    image: local/llama-cpp:latest
    command: >
      python3 -m llama_cpp.server
      --model /models/embed/${LOCAL_EMBED_MODEL_FILE:-embeddinggemma-300m-qat-Q4_0.gguf}
      --model_alias ${EMBEDDING_MODEL:-text-embedding-nomic-embed-text-v1.5}
      --host 0.0.0.0
      --port 8080
      --embedding true
      --n_ctx ${EMBED_CONTEXT_SIZE:-2048}
      --n_gpu_layers -1
      --use_mlock true
      --api_key ${EMBEDDING_API_KEY:-local-embed}
    environment:
      LLAMA_CUBLAS: "1"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ${LOCAL_EMBED_MODEL_DIR:-./models/embed}:/models/embed:ro
    ports:
      - "${EMBED_HOST_PORT:-8011}:8080"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    depends_on:
      - rag-backend
    ports:
      - "${FRONTEND_PORT:-5173}:80"
