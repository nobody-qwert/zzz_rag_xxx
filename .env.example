LLM_BASE_URL=http://llm:8000/v1
LLM_API_KEY=local-llm
LLM_MODEL=qwen/qwen3-4b-2507
EMBEDDING_BASE_URL=http://embed:8080/v1
EMBEDDING_API_KEY=local-embed
EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5

BACKEND_PORT=8000
FRONTEND_PORT=5173

DATA_DIR=/data
INDEX_DIR=/indices
MINERU_DEVICE_MODE=cuda

# MinerU parsing controls
# Use 'mineru' for rich OCR+layout, 'pymupdf' for fastest text-only, or 'auto' to use PyMuPDF when text per page is sufficient
PARSER_MODE=mineru
MIN_PYMUPDF_CHARS_PER_PAGE=300
MINERU_PARSE_METHOD=ocr
MINERU_LANG=en
MINERU_TABLE_ENABLE=true
MINERU_FORMULA_ENABLE=true
MINERU_WARMUP_ON_STARTUP=false

# Optional: set 1 to fetch model weights at build time (Docker build must have network access)
PREFETCH_MODELS=0

# Local model serving (llama.cpp containers)
# Model alias is what the backend calls; local file is the GGUF served by llama.cpp
LOCAL_QWEN_MODEL_DIR=./models/llm
LOCAL_QWEN_MODEL_FILE=Qwen3-4B-Instruct-2507-Q8_0.gguf
LOCAL_EMBED_MODEL_DIR=./models/embed
LOCAL_EMBED_MODEL_FILE=nomic-embed-text-v1.5.Q8_0.gguf
LLM_CONTEXT_SIZE=10000
EMBED_CONTEXT_SIZE=2048
LLM_HOST_PORT=8010
EMBED_HOST_PORT=8011
