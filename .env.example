COMPOSE_PROFILES=llm_small

# Active LLM endpoint (points backend to whichever profile you start)
LLM_ENDPOINT=llm_small
LLM_BASE_URL=http://${LLM_ENDPOINT}:8000/v1
LLM_API_KEY=local-llm

# Embeddings
EMBEDDING_BASE_URL=http://embed:8080/v1
EMBEDDING_API_KEY=local-embed
EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5
EMBEDDING_BATCH_SIZE=1

BACKEND_PORT=8000
FRONTEND_PORT=5173

# Shared app data root and subdirectories
HOST_APP_DATA=./app_data
APP_DATA_ROOT=/app_data
DATA_DIR=${APP_DATA_ROOT}/docs
INDEX_DIR=${APP_DATA_ROOT}/runtime
DOC_STORE_PATH=${APP_DATA_ROOT}/rag_meta.db
OCR_OUTPUT_DIR=${INDEX_DIR}/ocr_outputs
OCR_INCOMING_DIR=${INDEX_DIR}/incoming
OCR_WARMUP_DIR=${INDEX_DIR}/warmup
HF_HOME=${INDEX_DIR}/caches/hf
TRANSFORMERS_CACHE=${INDEX_DIR}/caches/hf
ULTRALYTICS_HOME=${INDEX_DIR}/caches/ultralytics

# Parser controls
OCR_PARSER_KEY=mineru
OCR_MODULE_URL=http://ocr-module:8000
OCR_MODULE_TIMEOUT=1200
OCR_STATUS_POLL_INTERVAL=5
CHUNK_SIZE=200
CHUNK_OVERLAP=60
LARGE_CHUNK_SIZE=1800
LARGE_CHUNK_LEFT_OVERLAP=100
LARGE_CHUNK_RIGHT_OVERLAP=100

# OCR module (MinerU) settings
MINERU_DEVICE_MODE=cuda
MINERU_PARSE_METHOD=ocr
MINERU_LANG=en
MINERU_TABLE_ENABLE=true
MINERU_FORMULA_ENABLE=true
MINERU_WARMUP_ON_STARTUP=true
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Retrieval controls
MIN_CONTEXT_SIMILARITY=0.6

# Optional: set 1 to fetch model weights at build time (Docker build must have network access)
PREFETCH_MODELS=0

# Local model serving (llama.cpp containers)
# Model alias is what the backend calls; local file is the GGUF served by llama.cpp
EMBED_MODEL_DIR=./models/embed
EMBED_MODEL_FILE=nomic-embed-text-v1.5.Q8_0.gguf
EMBED_CONTEXT_SIZE=2048

# Host ports
LLM_HOST_PORT=8010
EMBED_HOST_PORT=8011
