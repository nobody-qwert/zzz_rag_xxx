LLM_BASE_URL=http://llm:8000/v1
LLM_API_KEY=local-llm
LLM_MODEL=qwen/qwen3-4b-2507
EMBEDDING_BASE_URL=http://embed:8080/v1
EMBEDDING_API_KEY=local-embed
EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5
EMBEDDING_BATCH_SIZE=1

BACKEND_PORT=8000
FRONTEND_PORT=5173

# Shared app data root and subdirectories
HOST_APP_DATA=./app_data
APP_DATA_ROOT=/app_data
DATA_DIR=${APP_DATA_ROOT}/docs
INDEX_DIR=${APP_DATA_ROOT}/runtime
DOC_STORE_PATH=${APP_DATA_ROOT}/rag_meta.db
OCR_OUTPUT_DIR=${INDEX_DIR}/ocr_outputs
OCR_INCOMING_DIR=${INDEX_DIR}/incoming
OCR_WARMUP_DIR=${INDEX_DIR}/warmup
HF_HOME=${INDEX_DIR}/caches/hf
TRANSFORMERS_CACHE=${INDEX_DIR}/caches/hf
ULTRALYTICS_HOME=${INDEX_DIR}/caches/ultralytics

# Parser controls
OCR_PARSER_KEY=mineru
OCR_MODULE_URL=http://ocr-module:8000
OCR_MODULE_TIMEOUT=1200
OCR_STATUS_POLL_INTERVAL=5
CHUNK_SIZE=200
CHUNK_OVERLAP=60
LARGE_CHUNK_SIZE=1600
LARGE_CHUNK_LEFT_OVERLAP=100
LARGE_CHUNK_RIGHT_OVERLAP=100
SMALL_CHUNK_CONFIG_ID=chunk-small
SMALL_CHUNK_CONFIG_NAME=Small window
LARGE_CHUNK_CONFIG_ID=chunk-large
LARGE_CHUNK_CONFIG_NAME=Large window

# OCR module (MinerU) settings
MINERU_DEVICE_MODE=cuda
MINERU_PARSE_METHOD=ocr
MINERU_LANG=en
MINERU_TABLE_ENABLE=true
MINERU_FORMULA_ENABLE=true
MINERU_WARMUP_ON_STARTUP=true

# Retrieval controls
MIN_CONTEXT_SIMILARITY=0.35

# Optional: set 1 to fetch model weights at build time (Docker build must have network access)
PREFETCH_MODELS=0

# Local model serving (llama.cpp containers)
# Model alias is what the backend calls; local file is the GGUF served by llama.cpp
LOCAL_QWEN_MODEL_DIR=./models/llm
LOCAL_QWEN_MODEL_FILE=Qwen3-4B-Instruct-2507-Q8_0.gguf
LOCAL_EMBED_MODEL_DIR=./models/embed
LOCAL_EMBED_MODEL_FILE=nomic-embed-text-v1.5.Q8_0.gguf
LLM_CONTEXT_SIZE=10000
EMBED_CONTEXT_SIZE=2048
LLM_HOST_PORT=8010
EMBED_HOST_PORT=8011
